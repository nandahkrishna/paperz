import csv
from pathlib import Path
from typing import List, Dict
import os
from dotenv import load_dotenv
from supabase import create_client, Client
from tqdm import tqdm

BATCH_SIZE = 100  # Number of records to insert at once

class SupabaseImporter:
    def __init__(self, venues_path: str, papers_path: str):
        # Load environment variables
        load_dotenv()
        
        # Initialize Supabase client
        url: str = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
        key: str = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')
        if not url or not key:
            raise ValueError("Missing SUPABASE_URL or SUPABASE_KEY in .env file")
        
        self.supabase: Client = create_client(url, key)
        self.venues_path = Path(venues_path)
        self.papers_path = Path(papers_path)
        
        # Verify files exist
        if not self.venues_path.exists():
            raise FileNotFoundError(f"Venues file not found: {venues_path}")
        if not self.papers_path.exists():
            raise FileNotFoundError(f"Papers file not found: {papers_path}")

    def read_csv(self, filepath: Path) -> List[Dict]:
        """Read CSV file and return list of dictionaries"""
        data = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Convert empty strings to None
                processed_row = {k: (v if v != '' else None) for k, v in row.items()}
                
                # Convert string lists (authors) to actual lists
                if 'authors' in processed_row and processed_row['authors']:
                    processed_row['authors'] = processed_row['authors'].split('|')
                
                # Convert year to integer if present
                if 'year' in processed_row and processed_row['year']:
                    processed_row['year'] = int(processed_row['year'])
                
                data.append(processed_row)
                
        return data

    def batch_data(self, data: List[Dict], batch_size: int) -> List[List[Dict]]:
        """Split data into batches"""
        return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    def import_venues(self) -> Dict[str, str]:
        """Import venues data to Supabase and return mapping of (abbrev,year) to id"""
        print("Importing venues...")
        venues = self.read_csv(self.venues_path)
        venue_mapping = {}
        
        # Split into batches
        batches = self.batch_data(venues, BATCH_SIZE)
        
        with tqdm(total=len(venues), desc="Venues", unit="venue", leave=False) as pbar:
            for batch in batches:
                try:
                    # Prepare batch data
                    batch_data = [{
                        'name': venue['name'],
                        'abbrev': venue['abbrev'],
                        'year': venue['year'],
                    } for venue in batch]
                    
                    # Insert batch
                    result = self.supabase.table('venues').upsert(
                        batch_data,
                        on_conflict='abbrev,year'
                    ).execute()
                    
                    # Update mapping
                    for row in result.data:
                        key = (row['abbrev'], row['year'])
                        venue_mapping[key] = row['id']
                    
                    pbar.update(len(batch))
                    
                except Exception as e:
                    print(f"\nError importing venue batch: {str(e)}")
        
        print(f"Imported {len(venues)} venues")
        return venue_mapping

    def import_papers(self, venue_mapping: Dict[str, str]) -> None:
        """Import papers data to Supabase"""
        print("Importing papers...")
        papers = self.read_csv(self.papers_path)
        
        # Split into batches
        batches = self.batch_data(papers, BATCH_SIZE)
        
        with tqdm(total=len(papers), desc="Papers", unit="paper", leave=False) as pbar:
            for batch in batches:
                try:
                    # Prepare batch data with venue_ids
                    batch_data = []
                    for paper in batch:
                        venue_key = (paper['venue_abbrev'], int(paper['venue_year']))  # Convert to int here
                        venue_id = venue_mapping.get(venue_key)
                        
                        if not venue_id:
                            print(f"\nVenue not found for paper: {paper['title']}")
                            continue
                        
                        batch_data.append({
                            'title': paper['title'],
                            'authors': paper['authors'],
                            'abstract': paper['abstract'],
                            'venue_id': venue_id,
                            'pdf_url': paper['pdf_url'],
                            'code_url': paper['code_url']
                        })
                    
                    # Insert batch
                    if batch_data:  # Only insert if we have valid papers
                        self.supabase.table('papers').upsert(
                            batch_data,
                            on_conflict='venue_id,normalized_title'
                        ).execute()
                    
                    pbar.update(len(batch))
                    
                except Exception as e:
                    print(f"\nError importing paper batch: {str(e)}")
        
        print(f"Imported {len(papers)} papers")

    def import_all(self) -> None:
        """Import all data to Supabase"""
        # First import venues and get mapping
        venue_mapping = self.import_venues()
        # Then import papers using venue mapping
        self.import_papers(venue_mapping)
        print("Import completed!")

if __name__ == "__main__":
    # Get path relative to the script location
    script_dir = Path(__file__).parent
    root_dir = script_dir.parent
    
    venues_path = root_dir / "dumps/neurips/venues.csv"
    papers_path = root_dir / "dumps/neurips/papers.csv"
    
    importer = SupabaseImporter(
        venues_path=venues_path,
        papers_path=papers_path
    )
    importer.import_all()